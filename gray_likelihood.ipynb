{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining all the global variables in this cell\n",
    "'''\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "IMG_DEPTH = 1\n",
    "\n",
    "G_WIN_SIZE= 12\n",
    "G_DIM = G_WIN_SIZE*G_WIN_SIZE*IMG_DEPTH\n",
    "\n",
    "STD_VAR = 0.11\n",
    "\n",
    "LOC_DIM = 2 # the number of dimensions for the locations are just x and y so 2\n",
    "GLIMPSE_FC1 = 256\n",
    "GLIMPSE_FC2 = 512\n",
    "\n",
    "LSTM_HIDDEN = 512\n",
    "\n",
    "NUM_GLIMPSES = 6\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "BASE_OUT = 1\n",
    "SCALE = 3\n",
    "PAD_SIZE = G_WIN_SIZE * (2 ** (SCALE-1))\n",
    "\n",
    "NUM_EPISODES = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "# from config import *\n",
    "# from model import *\n",
    "\n",
    "\n",
    "'''\n",
    "Expecting means and locs to be of dimension [time_steps, batch_size, no_locations]\n",
    "'''\n",
    "def calc_likelihood(means, locs, sigma):\n",
    "\n",
    "    means = tf.stack(means)\n",
    "    locs = tf.stack(locs)\n",
    "    dist = tf.contrib.distributions.Normal(means, sigma)\n",
    "    pdf_val = dist.log_prob(locs)\n",
    "#     print(pdf_val.shape)\n",
    "    likelihood = tf.reduce_sum(pdf_val, 2)\n",
    "\n",
    "    return tf.transpose(likelihood)\n",
    "\n",
    "\n",
    "def read_and_decode(filename_queue, alter=True):\n",
    "    with tf.name_scope('read_and_decode'):\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "        features = tf.parse_single_example(serialized_example, features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string)\n",
    "        })\n",
    "\n",
    "        label = tf.cast(features['label'], tf.int32)\n",
    "        label = tf.one_hot(label, depth=10)\n",
    "        print(label.shape)\n",
    "        # label = tf.reshape(label, [1])\n",
    "\n",
    "        image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "\n",
    "        # Convert back to image shape\n",
    "        image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "        print(image.shape)\n",
    "\n",
    "        image = tf.cast(image, tf.float32)\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from config import *\n",
    "\n",
    "'''\n",
    "This file contains the model used for the project\n",
    "'''\n",
    "\n",
    "class Model:\n",
    "    # The inputs passed to the model and the batch size are the class variables used by all the functions\n",
    "    # inputs are 4D tensors [batch_size, HEIGHT, WIDTH, CHANNELS]\n",
    "    # batch_size is a scalar values representing the number of passed in the inputs tensor\n",
    "    # Collecting the mean values used by the model with collect_means\n",
    "    # Collecting the locations using collect_locs\n",
    "    def __init__(self, inputs, b_size):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = b_size * NUM_EPISODES\n",
    "        print(self.batch_size)\n",
    "        self.collect_locs = []\n",
    "        self.collect_means = []\n",
    "        \n",
    "\n",
    "    # This function calculated the inital locations, then build the LSTM cell \n",
    "    # the output of this function is the classifier output of the last LSTM cell \n",
    "    # class_outs is a 2D tensor of dimenstions [batch_size, number_of_claases]\n",
    "    def __call__(self):\n",
    "        initial_locs = tf.zeros([self.batch_size, LOC_DIM])\n",
    "        \n",
    "        input_lstm = self.glimpse_network(self.inputs, initial_locs)\n",
    "\n",
    "        collect_outputs= []\n",
    "        baselines = []\n",
    "        prev_output = tf.zeros([self.batch_size, LSTM_HIDDEN])\n",
    "        prev_state = tf.zeros([self.batch_size, LSTM_HIDDEN])\n",
    "\n",
    "        curr_out, next_state = self.lstm_layer(prev_output, prev_state, input_lstm)\n",
    "        prev_state = next_state\n",
    "        prev_output = self.next_location(curr_out, False)\n",
    "        \n",
    "        for i in range(NUM_GLIMPSES):\n",
    "            curr_out, next_state = self.lstm_layer(prev_output, prev_state, input_lstm)\n",
    "            changed = tf.concat([tf.ones([self.batch_size, 1]), curr_out], 1)\n",
    "            collect_outputs.append(changed)\n",
    "            base = self.baseline_layer(curr_out, GLIMPSE_FC2, BASE_OUT, 'baseline')\n",
    "            baselines.append(base)\n",
    "            prev_output = self.next_location(curr_out, True)\n",
    "            prev_state = next_state\n",
    "            if i == NUM_GLIMPSES-1:\n",
    "                class_outs = self.fc_layer(curr_out, GLIMPSE_FC2, NUM_CLASSES, 'softmax', None)\n",
    "\n",
    "\n",
    "        return baselines, class_outs, self.collect_means, self.collect_locs\n",
    "\n",
    "\n",
    "    def baseline_layer(self, image, in_size, out_size, name):\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            weights = tf.get_variable(\"weights\", [in_size, out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            biases = tf.get_variable(\"biases\", [out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            y = tf.nn.tanh(tf.add(tf.matmul(image, weights), biases))\n",
    "#             y = tf.stop_gradient(y)\n",
    "\n",
    "            return y\n",
    "\n",
    "    \n",
    "\n",
    "    # This function is called by the current LSTM cell to get inputs to the next cell\n",
    "    # next_inputs are of dimension [batch_size, 256]\n",
    "    def next_location(self, prev_inputs, is_first):\n",
    "        with tf.variable_scope('next_loc', reuse=tf.AUTO_REUSE):\n",
    "            weights = tf.get_variable(\"weights\", [GLIMPSE_FC2+1, LOC_DIM], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            prev_inputs = tf.concat([tf.ones([self.batch_size, 1]), prev_inputs], 1)\n",
    "            means = tf.clip_by_value(tf.matmul(prev_inputs, weights), -0.5, 0.5)\n",
    "\n",
    "#             dist = tf.contrib.distributions.Normal(means, STD_VAR)\n",
    "#             locs = tf.stop_gradient(tf.squeeze(dist.sample(1)))\n",
    "#             print(locs.shape)\n",
    "            locs = means\n",
    "            if is_first:\n",
    "                self.collect_locs.append(locs)\n",
    "                self.collect_means.append(means)\n",
    "\n",
    "            next_inputs = self.glimpse_network(self.inputs, locs)\n",
    "            \n",
    "            return next_inputs\n",
    "\n",
    "\n",
    "    # This function has the glimpse network where the locations are processed \n",
    "    # output is a 2D tensor of dimension [batch_size, 256]\n",
    "    def glimpse_network(self, input_img, locations):\n",
    "\n",
    "        loc_out1 = self.fc_layer(locations, LOC_DIM, 256, 'lc1', tf.nn.tanh)\n",
    "        loc_out2 = self.fc_layer(loc_out1, 256, 512, 'lc2', tf.nn.tanh)\n",
    "\n",
    "        glimpses = tf.image.extract_glimpse(input_img, [G_WIN_SIZE,G_WIN_SIZE], \n",
    "                                                locations, centered=True, normalized=True)\n",
    "        # print(input_img.shape)\n",
    "        glimpses_2 = tf.image.extract_glimpse(input_img, [24,24], \n",
    "                                                locations, centered=True, normalized=True)\n",
    "        # glimpses_3 = tf.image.extract_glimpse(input_img, [28,28], \n",
    "        #                                         locations, centered=True, normalized=True)\n",
    "        \n",
    "        glimpses = tf.squeeze(glimpses)\n",
    "        glimpses_2 = tf.squeeze(tf.image.resize_images(glimpses_2, [12,12]))\n",
    "        # glimpses_3 = tf.squeeze(tf.image.resize_images(glimpses_3, [8,8]))\n",
    "\n",
    "        glimpses = tf.reshape(glimpses, [-1, 12*12*1])\n",
    "        glimpses_2 = tf.reshape(glimpses_2, [-1, 144])\n",
    "        # glimpses_3 = tf.reshape(glimpses_3, [-1, 64])\n",
    "\n",
    "        glimpses = tf.concat([glimpses, glimpses_2], 1)\n",
    "\n",
    "        # glimpses = tf.reshape(glimpses, [-1, G_WIN_SIZE*G_WIN_SIZE*IMG_DEPTH])\n",
    "\n",
    "        g_out1 = self.fc_layer(glimpses, 288, 512, 'g1', tf.nn.tanh)\n",
    "        g_out2 = self.fc_layer(g_out1, 512, 512, 'g2', tf.nn.tanh)\n",
    "\n",
    "\n",
    "        return tf.nn.relu(loc_out2 + g_out2)\n",
    "\n",
    "\n",
    "    # general template for a fully connected layer used by the model\n",
    "    # output dimensions are [batch_size, out_size]\n",
    "    def fc_layer(self, image, in_size, out_size, name, activation):\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            weights = tf.get_variable(\"weights\", [in_size, out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            biases = tf.get_variable(\"biases\", [out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            y = tf.add(tf.matmul(image, weights), biases)\n",
    "\n",
    "#             y = tf.layers.batch_normalization(y, training=True)\n",
    "            \n",
    "            if activation is not None:\n",
    "                y = activation(y)\n",
    "\n",
    "            return y\n",
    "    \n",
    "    \n",
    "    def lstm_layer(self, last_output, last_state, curr_input):\n",
    "        \n",
    "        with tf.variable_scope('lstm_cell', reuse=tf.AUTO_REUSE):\n",
    "            whprev = tf.get_variable(\"whprev\", [GLIMPSE_FC2, GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            wx = tf.get_variable(\"wxcurr\", [GLIMPSE_FC2, GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            wf = tf.get_variable(\"wf\", [GLIMPSE_FC2, GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bf = tf.get_variable(\"bf\", [GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            wi = tf.get_variable(\"wi\", [GLIMPSE_FC2, GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bi = tf.get_variable(\"bi\", [GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            wc = tf.get_variable(\"wc\", [GLIMPSE_FC2, GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bc = tf.get_variable(\"bc\", [GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            wo = tf.get_variable(\"wo\", [GLIMPSE_FC2, GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bo = tf.get_variable(\"bo\", [GLIMPSE_FC2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            main_mix = tf.matmul(last_output, whprev)+ tf.matmul(curr_input, wx)\n",
    "            \n",
    "            ft = tf.nn.sigmoid(tf.add(tf.matmul(main_mix, wf), bf))\n",
    "            \n",
    "            it = tf.nn.sigmoid(tf.add(tf.matmul(main_mix, wi), bi))\n",
    "            \n",
    "            cbart = tf.nn.tanh(tf.add(tf.matmul(main_mix, wc), bc))\n",
    "             \n",
    "            ct = tf.multiply(ft, last_state) + tf.multiply(it, cbart)\n",
    "            \n",
    "            ot = tf.nn.sigmoid(tf.add(tf.matmul(main_mix, wo), bo))\n",
    "            \n",
    "            ht = tf.multiply(ot, tf.nn.tanh(ct))\n",
    "            \n",
    "            return ht, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(32, 32, 1)\n",
      "100\n",
      "Step 550: A=0.5199999809265137 d=-3.303061008453369 c=4.688730716705322 b=1.8929040431976318\n",
      "Step 1100: A=0.4300000071525574 d=-3.4584290981292725 c=3.339381694793701 b=2.046644926071167\n",
      "Step 1650: A=0.3799999952316284 d=-3.067120313644409 c=5.256348133087158 b=1.6526150703430176\n",
      "Step 2200: A=0.4000000059604645 d=-2.648296356201172 c=6.410726547241211 b=1.2963687181472778\n",
      "Step 2750: A=0.38999998569488525 d=-3.572054624557495 c=8.223984718322754 b=2.1597421169281006\n",
      "Step 3300: A=0.44999998807907104 d=-3.730309247970581 c=5.269430160522461 b=2.343402862548828\n",
      "Step 3850: A=0.36000001430511475 d=-3.4375429153442383 c=5.771368980407715 b=2.010227918624878\n",
      "Step 4400: A=0.3499999940395355 d=-3.448655843734741 c=5.493671894073486 b=2.0188536643981934\n",
      "Step 4950: A=0.4300000071525574 d=-3.0171427726745605 c=4.702121257781982 b=1.616219401359558\n",
      "Step 5500: A=0.3799999952316284 d=-2.8335392475128174 c=5.039307117462158 b=1.4449281692504883\n",
      "Step 6050: A=0.3799999952316284 d=-2.795325994491577 c=4.978837966918945 b=1.4125347137451172\n",
      "Step 6600: A=0.4300000071525574 d=-2.975454092025757 c=4.576298236846924 b=1.5786062479019165\n",
      "Step 7150: A=0.28999999165534973 d=-2.5426840782165527 c=5.669974327087402 b=1.179719090461731\n",
      "Step 7700: A=0.49000000953674316 d=-3.829498767852783 c=4.0797200202941895 b=2.458745002746582\n",
      "Step 8250: A=0.3799999952316284 d=-3.5506021976470947 c=4.953405380249023 b=2.134427785873413\n",
      "Step 8800: A=0.3499999940395355 d=-3.472193479537964 c=5.169912815093994 b=2.043388843536377\n",
      "Step 9350: A=0.38999998569488525 d=-3.5763447284698486 c=4.871082305908203 b=2.164360523223877\n",
      "Step 9900: A=0.3700000047683716 d=-3.5271706581115723 c=5.011923789978027 b=2.106948137283325\n",
      "Step 10450: A=0.36000001430511475 d=-3.494574546813965 c=5.122634410858154 b=2.0697743892669678\n",
      "Step 11000: A=0.49000000953674316 d=-3.8271520137786865 c=4.0860090255737305 b=2.4560391902923584\n",
      "Step 11550: A=0.3700000047683716 d=-3.523421049118042 c=5.011636257171631 b=2.102965831756592\n",
      "Step 12100: A=0.46000000834465027 d=-3.7557969093322754 c=4.317381381988525 b=2.3730416297912598\n",
      "Step 12650: A=0.47999998927116394 d=-3.808638334274292 c=4.151299476623535 b=2.4344470500946045\n",
      "Step 13200: A=0.3799999952316284 d=-2.8671488761901855 c=7.326826095581055 b=1.4737790822982788\n",
      "Step 13750: A=0.36000001430511475 d=-3.4990885257720947 c=10.202935218811035 b=2.0745291709899902\n",
      "Step 14300: A=0.4399999976158142 d=-3.7085683345794678 c=5.867677211761475 b=2.317943811416626\n",
      "Step 14850: A=0.38999998569488525 d=-2.4239985942840576 c=5.977263927459717 b=1.1229920387268066\n",
      "Step 15400: A=0.3499999940395355 d=-3.477254867553711 c=6.218254566192627 b=2.0486867427825928\n",
      "Step 15950: A=0.4099999964237213 d=-3.631113052368164 c=5.475540637969971 b=2.2278168201446533\n",
      "Step 16500: A=0.4399999976158142 d=-3.565992832183838 c=5.217920303344727 b=2.161724805831909\n",
      "Step 17050: A=0.4099999964237213 d=-3.627051830291748 c=5.490567684173584 b=2.2233762741088867\n",
      "Step 17600: A=0.4099999964237213 d=-3.36889386177063 c=5.440395355224609 b=1.9513500928878784\n",
      "Step 18150: A=0.44999998807907104 d=-2.23584246635437 c=4.975069999694824 b=1.000462293624878\n",
      "Step 18700: A=0.47999998927116394 d=-3.799456834793091 c=4.765054225921631 b=2.4239253997802734\n",
      "Step 19250: A=0.4099999964237213 d=-3.6259782314300537 c=5.421377658843994 b=2.2222037315368652\n",
      "Step 19800: A=0.4099999964237213 d=-3.632324695587158 c=5.425725936889648 b=2.229142189025879\n",
      "Step 20350: A=0.3700000047683716 d=-3.5284130573272705 c=5.808915615081787 b=2.1082680225372314\n",
      "Step 20900: A=0.5400000214576721 d=-3.9458227157592773 c=4.215089797973633 b=2.5934739112854004\n",
      "Step 21450: A=0.44999998807907104 d=-3.732053518295288 c=5.047969818115234 b=2.345363140106201\n",
      "Step 22000: A=0.3499999940395355 d=-3.4607365131378174 c=5.991827487945557 b=2.0314252376556396\n",
      "Step 22550: A=0.5 d=-3.862968921661377 c=4.679354667663574 b=2.4976251125335693\n",
      "Step 23100: A=0.4300000071525574 d=-3.082880973815918 c=5.183835029602051 b=1.6766129732131958\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train function\n",
    "Processes inputs in mini-batchs \n",
    "Builds the model and trains the parameters for predetermined number of times \n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os \n",
    "# from config import *\n",
    "# from model import *\n",
    "# from util import *\n",
    "from tensorflow.python.framework import ops\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "def train(batch_size, epochs, log, output):\n",
    "    # Read arguments\n",
    "    ops.reset_default_graph()\n",
    "    filename_queue = tf.train.string_input_producer(['/N/u/ramyarao/project/cifar_model/cifar_gray_train.tfrecords'])\n",
    "    image, label = read_and_decode(filename_queue)\n",
    "    batch = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=500, num_threads=2, min_after_dequeue=250)\n",
    "\n",
    "    # placeholders for the input and labels\n",
    "    X = tf.placeholder(tf.float32, [None, IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH], name='X')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "\n",
    "    # Model instantiated and called for processing the inputs\n",
    "    model = Model(X, batch_size)\n",
    "    b_t, y_hat, means, locs = model()\n",
    "\n",
    "    class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y,\n",
    "        logits=y_hat\n",
    "    ))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y, 1))\n",
    "    # correct_prediction = tf.equal(y_hat, y)\n",
    "    reward_last_step = tf.expand_dims(tf.cast(correct_prediction, tf.float32), 1)\n",
    "    rewards = tf.tile(reward_last_step, (1, NUM_GLIMPSES)) \n",
    "\n",
    "    log_likelihood = calc_likelihood(means, locs, STD_VAR)\n",
    "    penalty = rewards - b_t\n",
    "\n",
    "    del_j = tf.reduce_mean(log_likelihood * penalty)\n",
    "\n",
    "    baseline_loss = tf.reduce_mean(tf.square((rewards - b_t)))\n",
    "    \n",
    "    loss = -del_j + class_loss + baseline_loss\n",
    "\n",
    "    var_list = tf.trainable_variables()\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt = tf.train.AdamOptimizer(1e-3).minimize(loss, var_list=var_list)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "    \n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#     sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(log, sess.graph)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        # training the model for a predetermined number of epochs\n",
    "        for i in range(epochs):\n",
    "            batch_x, batch_lbl = sess.run(batch)\n",
    "            batch_x = np.tile(batch_x, [NUM_EPISODES, 1, 1, 1])\n",
    "            batch_lbl = np.tile(batch_lbl, [NUM_EPISODES, 1])\n",
    "            sess.run(opt, feed_dict={X: batch_x, y: batch_lbl})\n",
    "            print(sess.run(tf.argmax(y_hat, 1)))\n",
    "#             print(sess.run([locs[-1], means[-1]], feed_dict={X: batch_x, y: batch_lbl}))\n",
    "            acc, d, c, b = sess.run([accuracy, -del_j, class_loss, baseline_loss] ,feed_dict={X: batch_x, y: batch_lbl})\n",
    "\n",
    "\n",
    "            s = sess.run(merged_summary, feed_dict={X: batch_x, y: batch_lbl})\n",
    "            writer.add_summary(s, i)\n",
    "\n",
    "            if (i+1) % 550 == 0:\n",
    "                print('Step {}: A={} d={} c={} b={}'.format(i+1, acc, d, c, b))\n",
    "\n",
    "            if (((i+1) % 550 == 0) and (acc > 0.90)):\n",
    "                # Please change the directory here to save the model in a different location\n",
    "                params = saver.save(sess, '/N/u/ramyarao/project/model/{}_{}.ckpt'.format(output, i+1))\n",
    "                print('Model saved: {}'.format(params))\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    return\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(100, 550*100000, 'logs', 'model')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train function\n",
    "Processes inputs in mini-batchs \n",
    "Builds the model and trains the parameters for predetermined number of times \n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os \n",
    "# from config import *\n",
    "# from model import *\n",
    "# from util import *\n",
    "from tensorflow.python.framework import ops\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "def test(batch_size, epochs, log, output):\n",
    "    # Read arguments\n",
    "    ops.reset_default_graph()\n",
    "    filename_queue = tf.train.string_input_producer(['/N/u/ramyarao/project/cifar_model/cifar_gray_test.tfrecords'])\n",
    "    image, label = read_and_decode(filename_queue)\n",
    "    batch = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=500, num_threads=2, min_after_dequeue=250)\n",
    "\n",
    "    # placeholders for the input and labels\n",
    "    X = tf.placeholder(tf.float32, [None, IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH], name='X')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "\n",
    "    # Model instantiated and called for processing the inputs\n",
    "    model = Model(X, batch_size)\n",
    "    b_t, y_hat, means, locs = model()\n",
    "\n",
    "    temp1 = tf.argmax(y_hat, 1)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "        new_saver = tf.train.import_meta_graph('/N/u/ramyarao/project/model/'+output + '.meta')\n",
    "        new_saver.restore(sess, '/N/u/ramyarao/project/model/'+ output)\n",
    "\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(log, sess.graph)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        # training the model for a predetermined number of epochs\n",
    "        for i in range(epochs):\n",
    "            batch_x, batch_lbl = sess.run(batch)\n",
    "            \n",
    "            print(sess.run([temp1], feed_dict={X: batch_x, y: batch_lbl}))\n",
    "            print(sess.run([tf.argmax(batch_lbl,1)], feed_dict={X: batch_x, y: batch_lbl}))\n",
    "            acc = sess.run(accuracy, feed_dict={X: batch_x, y: batch_lbl})\n",
    "\n",
    "            print('Step {}: {}'.format(i+1, acc))\n",
    "\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test(200, 3, 'logs', 'model_6270.ckpt')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
